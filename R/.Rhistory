test1 <- term_matrix(dat$headline)
test2.tok <- tokens(dat$headline)
test2a <- dfm(test2.tok)
test2b.tok <- tokens(dat$headline, split_hyphens = T)
test2b <- dfm(test2b.tok)
dim(test1)
dim(test2a)
dim(test2b)
stylest1terms <- colnames(test1)
stylest2aterms <- colnames(test2a)
stylest2bterms <- colnames(test2b)
sort(unique(stylest1terms[!stylest1terms %in% stylest2aterms]))
#sort(unique(stylest1terms[!stylest1terms %in% stylest2bterms]))
sort(unique(stylest2aterms[!stylest2aterms %in% stylest1terms]))
#sort(unique(stylest2bterms[!stylest2bterms %in% stylest1terms]))
temp <- c("wars'-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
## looks like term_matrix() splits on the "'" character. Also separates "-"
## from any text that follows
temp <- c("wars'meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
## looks like term_matrix() splits on the "'" character. Also separates "-"
## from any text that follows
temp <- c("wars'-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("wars-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("wars'-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("wars'meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
## also breaks the hyphen away from the subsequent text when it is a leading hyphen
temp <- c("-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("'-meets-shakespeare")
term_matrix(temp)
dfm(tokens(temp))
temp <- c("wars'")
term_matrix(temp)
dfm(tokens(temp))
library(quanteda)
library(corpus)
load("novels_excerpts.RData")
## both quanteda and corpus will separate text from trailing hyphens
temp <- c("wars'")
term_matrix(temp)
dfm(tokens(temp))
temp="ever-growing"
corpus::term_matrix(temp)
library(quanteda)
dfm(tokens(temp))
temp="Christian's"
corpus::term_matrix(temp)
dfm(tokens(temp))
line <- "I went to the store --but forgot my wallet"
as.matrix(corpus::term_matrix(line))
as.matrix(quanteda::dfm(tokens(line)))
as.matrix(quanteda::dfm(tokens(line, split_hyphens = T)))
dat=read.csv("/Users/christianbaehr/Desktop/scrape_municipality_catcherrors/muni_data_Allegheny_1.csv", sep = ";")
View(dat)
t<- 1
length(t)
class(t)
is.integer(t)
t<- 1.5
as.integer(t)
t && 1
t %% 1
getwd()
load("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/data/novels_excerpts.RData")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
data(novels)
setwd("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/data")
data(novels_excerpts)
# show a snippet of the data
kable(novels[c(1,4,8), ]) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
library(stylest2)
library(quanteda)
# show a snippet of the data
kable(novels[c(1,4,8), ]) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
library(kableExtra)
# show a snippet of the data
kable(novels[c(1,4,8), ]) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
unique(novels$author)
novels_tok <- tokens(novels)
novels_tok <- tokens(novels$text)
docvars(novels_tok)["author"] <- novels$author
novels_dfm <- dfm(novels_tok)
docvars(novels_dfm)
novels_tok <- tokens(novels$text)
novels_dfm <- dfm(novels_tok)
unique(novels$author)
docvars(novels_dfm)["author"] <- novels$author
help(tokens)
novels_tok <- tokens(novels$text,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_separators = T,
split_hyphens = T)
novels_dfm <- dfm(novels_tok)
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
source("../R/stylest2_select_vocab.R")
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
source("../R/stylest2_select_vocab.R")
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
novels_tok <- tokens(novels$text,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_separators = T,
split_hyphens = T)
novels_dfm <- dfm(novels_tok)
unique(novels$author)
docvars(novels_dfm)["author"] <- novels$author
set.seed(1234)
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
source("../R/stylest2_fit.R")
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
source("../R/stylest2_predict.R")
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
help(warning)
suppressWarnings()
help("suppressWarnings")
suppressWarnings(warning("oopsy"))
warning("oopsy")
warn <- warning("Detected multiple texts with the same author. Collapsing to author-level dfm for stylest2_fit() function.")
warn
setwd("/Users/christianbaehr/Documents/GitHub/stylest2/R/")
library(stylest)
library(quanteda)
library(quanteda.corpora)
source("stylest2_select_vocab.R")
source("stylest2_fit.R")
source("stylest2_predict.R")
load("../data/novels_excerpts.RData")
novels$id <- 1:nrow(novels)
novelst <- tokens(novels$text)
novelsd <- dfm(novelst)
docvars(novelsd)["author"] <- novels$author
## fit stylest model over all SOTU observations.
s1_fit <- stylest_fit(x = novels$text,
speaker = novels$author)
setwd("/Users/christianbaehr/Documents/GitHub/stylest2/R/")
library(stylest)
library(quanteda)
library(quanteda.corpora)
source("stylest2_select_vocab.R")
source("stylest2_fit.R")
source("stylest2_predict.R")
## load State of the Union speeches
## this is a corpus object with speech text a docvar for President name
load("../data/sotu_speeches.RData")
## convert corpus object to quanteda dfm
## Note: we do not include any pre-processing options. This is with the purpose
## to evaluate the natural performance of stylest2 relative to stylest on an
## example dataset
sotu_dfm <- dfm(tokens(sotu))
## load State of the Union speeches
## this is a corpus object with speech text a docvar for President name
load("../data/sotu_speeches.RData")
setwd("/Users/christianbaehr/Documents/GitHub/stylest2_project/R/")
setwd("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/")
library(stylest)
library(quanteda)
library(quanteda.corpora)
source("stylest2_select_vocab.R")
source("stylest2_fit.R")
source("stylest2_predict.R")
## load State of the Union speeches
## this is a corpus object with speech text a docvar for President name
load("../data/sotu_speeches.RData")
setwd("/Users/christianbaehr/Documents/GitHub/stylest2/R/")
library(stylest)
library(quanteda)
library(quanteda.corpora)
source("stylest2_select_vocab.R")
source("stylest2_fit.R")
source("stylest2_predict.R")
## load State of the Union speeches
## this is a corpus object with speech text a docvar for President name
load("../data/sotu_speeches.RData")
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/stylest2/R/")
library(stylest)
library(quanteda)
library(quanteda.corpora)
source("stylest2_select_vocab.R")
source("stylest2_fit.R")
source("stylest2_predict.R")
## load State of the Union speeches
## this is a corpus object with speech text a docvar for President name
load("../data/sotu_speeches.RData")
## convert corpus object to quanteda dfm
## Note: we do not include any pre-processing options. This is with the purpose
## to evaluate the natural performance of stylest2 relative to stylest on an
## example dataset
sotu_dfm <- dfm(tokens(sotu))
## set authorship docvar name to "author"
docvars(sotu_dfm)["author"] <- docvars(sotu)["President"][[1]]
############################# stylest2_select_vocab #############################
## generate a random seed. Used by stylest2_select_vocab() to bin data into
## random "folds" to perform k-fold cross-validation.
num <- sample(1:1000, 1)
set.seed(num)
s1_terms <- stylest_select_vocab(x=as.character(sotu), speaker = docvars(sotu)["President"][[1]])
s1_terms$miss_pct
## miss rate in test fold, across cutoff hyperparameter values
set.seed(num)
s2_terms <- stylest2_select_vocab(dfm=sotu_dfm)
s2_terms$cv_missrate_results
warnings()
s2_terms_nowarn <- suppressWarnings(stylest2_select_vocab(dfm=sotu_dfm))
s2_terms_nowarn==s2_terms
all.equal(s2_terms_nowarn, s2_terms)
num <- sample(1:1000, 1)
set.seed(num)
s2_terms <- stylest2_select_vocab(dfm=sotu_dfm)
set.seed(num)
s2_terms_nowarn <- suppressWarnings(stylest2_select_vocab(dfm=sotu_dfm))
all.equal(s2_terms_nowarn, s2_terms)
rm(list = ls())
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_fit.R")
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_predict.R")
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_select_vocab.R")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(kableExtra)
library(stylest2)
library(stylest2)
library(quanteda)
data(novels)
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/data/novels_excerpts.RData")
load("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/data/novels_excerpts.RData")
# show a snippet of the data
kable(novels[c(1,4,8), ]) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
novels_tok <- tokens(novels$text)
novels_dfm <- dfm(novels_tok)
unique(novels$author)
docvars(novels_dfm)["author"] <- novels$author
novels_tok <- tokens(novels$text,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_separators = T,
split_hyphens = T)
novels_dfm <- dfm(novels_tok)
unique(novels$author)
docvars(novels_dfm)["author"] <- novels$author
set.seed(1234)
vocab_with_defaults <- stylest2_select_vocab(dfm = novels_dfm)
vocab_with_defaults$cutoff_pct_best
vocab_with_defaults$miss_pct
vocab_with_defaults$cv_missrate_results
vocab_with_defaults$cutoff_pcts
vocab_with_defaults$cutoff_candidates
# Number of folds
vocab_with_defaults$nfold
# Percentile with best prediction rate
vocab_with_defaults$cutoff_pct_best
# Rate of INCORRECTLY predicted speakers of held-out texts
vocab_with_defaults$cv_missrate_results
# Data on the setup:
# Percentiles tested
vocab_with_defaults$cutoff_candidates
# Number of folds
vocab_with_defaults$nfold
terms_90 <- stylest_terms(dfm = novels_dfm, cutoff = 90)
terms_90 <- stylest2_terms(dfm = novels_dfm, cutoff = 90)
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_select_vocab.R")
terms_90 <- stylest2_terms(dfm = novels_dfm, cutoff = 90)
terms_90
mod <- stylest2_fit(dfm = novels_dfm, terms = terms_90)
term_weights <- data.frame("word" = c("the", "and", "Floccinaucinihilipilification"),
"mean_distance" = c(0.1,0.2,0.001))
term_weights
mod <- stylest_fit(dfm = novels_dfm,
terms = terms_90,
term_weights = term_weights,
weight_varname = "mean_distance")
mod <- stylest2_fit(dfm = novels_dfm,  terms = terms_90, term_weights = term_weights, weight_varname = "mean_distance")
mod <- stylest2_fit(dfm = novels_dfm,  terms = terms_90, term_weights = term_weights)
mod <- stylest2_fit(dfm = novels_dfm,  terms = terms_90, term_weights = term_weights)
posterior <- stylest2_predict(dfm = novels_dfm, model = mod)
posterior <- stylest2_predict(dfm = novels_dfm, model = mod)
mod$term_weights
posterior <- stylest2_predict(dfm = novels_dfm, model = mod)
dfm=novels_dfm
model=mod
dfm_class <- class(dfm)
if ( dfm_class != 'dfm' | attr(dfm_class, 'package') != 'quanteda' ) {
stop('`dfm` must be a quanteda "dfm" object.')
}
if( attr(model, 'package') != 'stylest2' ) {
stop('`model` must be a "stylest2" model, fit using "stylest2_fit()".')
}
pred_docs_dfm <- quanteda::dfm_match(dfm, features=colnames(model$rate))
pred_docs_ntoken <- Matrix::rowSums(pred_docs_dfm)
speakers <- rownames(model$rate)
nspeaker <- length(unique(speakers))
if (is.null(prior)) {
log_prior <- rep(-log(nspeaker), nspeaker) # log(1/nspeaker)
} else {
log_prior <- log(prior)
}
prior=NULL
if (is.null(prior)) {
log_prior <- rep(-log(nspeaker), nspeaker) # log(1/nspeaker)
} else {
log_prior <- log(prior)
}
if (is.null(prior)) {
log_prior <- rep(-log(nspeaker), nspeaker) # log(1/nspeaker)
} else {
log_prior <- log(prior)
}
## we take the log of the speaker-specific rate for each term in the model,
## because it simplifies computation of the likelihood function
eta_tv <- log(model$rate)
eta_store <- eta_tv # store old etas for term_influence
if (!is.null(model$term_weights)) {
# make sure weights are in the same order for matrix multiplication
sorted_weights <- model$term_weights[colnames(eta_tv)]
for (i in 1:nrow(eta_tv)) {
eta_tv[i, ] <- eta_tv[i, ] * sorted_weights
}
}
model$term_weights
!is.null(model$term_weights)
colnames(eta_tv)
model$term_weights[colnames(eta_tv)]
model$term_weights
model$term_weights
colnames(eta_tv)
rownames(model$term_weights)
model$terms
colnames(eta_tv)
match(model$terms, colnames(eta_tv))
test <- c("upon", "is", "know")
match(model$terms, test)
match(test, model$terms)
model$term_weights[match(colnames(eta_tv), models$terms)]
match(colnames(eta_tv), model$terms)
match(colnames(eta_tv), model$terms)
match(test, model$terms)
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_predict.R")
posterior <- stylest2_predict(dfm = novels_dfm, model = mod)
rm(list = ls())
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_predict.R")
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_fit.R")
source("/Users/christianbaehr/Documents/GitHub/stylest2_project/stylest2_project/R/stylest2_select_vocab.R")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(kableExtra)
#library(stylest2)
library(quanteda)
data(novels)
# show a snippet of the data
kable(novels[c(1,4,8), ]) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
novels <- load("/Use")
help(stylest2)
??stylest2
remove.packages("stylest")
??stylest2
help(package="stylest2")
library(stylest2)
data(novels)
novels_dfm <- dfm(tokens(novels))
library(quanteda)
novels_dfm <- dfm(tokens(novels))
novels
novels_dfm <- dfm(novels)
class(novels)
novels_dfm <- dfm(tokens(novels$text))
best_cut <- stylest2_select_vocab(dfm=novels_dfm)
names(novels)
docvar(novels_dfm)["author"] <- novels$author
best_cut <- stylest2_select_vocab(dfm=novels_dfm)
docvars(novels_dfm)["author"] <- novels$author
best_cut <- stylest2_select_vocab(dfm=novels_dfm)
stylest2_terms(dfm = novels_dfm, cutoff=best_cut$cutoff_pct_best)
best_cut$cutoff_pct_best
gsub("%", "", "80%")
stylest2_select_vocab <- function(dfm, smoothing=0.5, cutoffs=c(50, 60, 70, 80, 90, 99),
nfold=5, terms=NULL, term_weights=NULL, fill=FALSE,
fill_weight=NULL, suppress_warning=TRUE) {
## check that `dfm` is of acceptable class and, if so, does it have the correct
## columns?
if(!inherits(dfm, 'dfm')) {
stop('`dfm` object must be of class "dfm".')
}
if(smoothing<0 | !is.numeric(smoothing) | length(smoothing)>1) {
stop('smoothing must be a non-negative numeric vector of length 1.')
}
if( !is.numeric(cutoffs) ) {
stop('cutoffs must be a numeric vector.')
} else if (any(cutoffs > 100 | cutoffs < 0)) {
stop('all values in cutoffs must be between 0 and 100')
}
if(nfold<1 ) {
stop('nfold must be at least 1')
} else if (nfold %% 1 != 0) {
stop('nfold must be an integer')
}
## extract authors from input data
if( !'author' %in% names(quanteda::docvars(dfm)) ) {
stop('Need to supply a docvar in `dfm` with text authorship with name "author".')
} else {
author_loc <- as.character(quanteda::docvars(dfm)['author'][,1])
}
if(length(author_loc) < 2) {
stop('Should be at least two authors in the corpus.')
}
if(any(is.na(author_loc))) {
stop('Detected missingness in the authorship variable. Documents with missing authorship should be omitted from the corpus.')
}
## if there are any docvars that dont contain "author"
if( any(colnames(quanteda::docvars(dfm)) != 'author') ) {
warning('Additional docvars detected other than authorship. These will be ignored.')
}
## reorder documents prior distributing folds
ndoc <- nrow(dfm)
## assign rows to folds
test_fold <- sample(rep(1:nfold, ceiling(ndoc / nfold)), ndoc)
## empty matrix to store missrate for each fold, cutoff
missrate <- matrix(NA, nrow = nfold, ncol = length(cutoffs))
colnames(missrate) <- paste0(cutoffs, "%")
rownames(missrate) <- 1:nfold
for(i in 1:nfold) {
## allocate all folds except i to training set, i to test set
test_set <- (test_fold == i)
train_set <- !test_set
X_test <- dfm[test_set, ]
X_train <- dfm[train_set, ]
## temporary dfm -- utility object to help retrieve terms with frequency >(cutoff/100)
temp <- dfm(X_train)
for(j in 1:length(cutoffs)) {
## select only the terms above the (j/100)th quantile of frequency
keep_terms <- colnames( quanteda::dfm_trim(temp, min_termfreq = (cutoffs[j]/100), termfreq_type = c("quantile")) )
## keep only the candidate terms
#X_train_sub <- tokens_select(X_train, pattern=keep_terms)
X_train_sub <- quanteda::dfm_select(X_train, pattern = keep_terms)
## fit the model to the terms above cutoff
if(suppress_warning) {
fit <- suppressWarnings(stylest2_fit(dfm=X_train_sub, smoothing=smoothing,
terms=terms, term_weights=term_weights,
fill_weight=fill_weight))
} else{
fit <- stylest2_fit(dfm=X_train_sub, smoothing=smoothing, terms=terms,
term_weights=term_weights, fill_weight=fill_weight)
}
## predict authors for the hold-out fold
prediction <- stylest2_predict(model=fit, dfm=X_test)
## calculate miss rate -- the percent of hold-out texts for which we get authorship
## wrong
missrate[i, j] <- 100 * mean(prediction$posterior$predicted != author_loc[test_set])
}
}
## collapse to the mean of missrates across folds
avg_missrate <- apply(missrate, 2, mean)
## retrieve the highest performing cutoff rate in terms of minimizing average misses
bestrate <- names(avg_missrate)[which.min(avg_missrate)]
bestrate <- gsub("%", "", bestrate)
return(
list(cutoff_pct_best = as.numeric(bestrate),
cutoff_candidates = cutoffs,
cv_missrate_results = missrate,
nfold = nfold)
)
}
best_cut <- stylest2_select_vocab(dfm=novels_dfm)
best_cut$cutoff_pct_best
stylest2_terms(dfm = novels_dfm, cutoff=best_cut$cutoff_pct_best)
data(novels)
novels_dfm <- dfm(tokens(novels$text))
docvars(novels_dfm)["author"] <- novels$author
stylest2_fit(dfm = novels_dfm)
data(novels)
novels_dfm <- dfm(tokens(novels$text))
docvars(novels_dfm)["author"] <- novels$author
mod <- stylest2_fit(novels_dfm)
stylest2_predict(dfm=novels_dfm, model=mod)
data(novels)
novels_dfm <- dfm(tokens(novels$text))
docvars(novels_dfm)["author"] <- novels$author
stylest2_select_vocab(dfm=novels_dfm)
